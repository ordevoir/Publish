
>[!abstract]-
> Случайные величины $X$ и $Y$ называются **независимыми**, если знание значения одной из них не даёт никакой информации о распределении другой. Формально это означает, что вероятность их совместного попадания в любые множества равна произведению вероятностей попадания каждой величины в своё множество по отдельности. Интуитивно, независимость означает отсутствие какой-либо вероятностной связи между случайными величинами – они "не влияют" друг на друга.
> 
> Случайные величины $X$ и $Y$ называются **зависимыми**, если между ними существует вероятностная связь, то есть знание значения одной величины изменяет наше представление о распределении другой. Это проявляется в том, что их совместное распределение не факторизуется в произведение маргинальных распределений. Зависимость может быть как функциональной (когда одна величина полностью определяется другой), так и статистической (когда связь носит вероятностный характер).

>[!addition]- Строгое определение
> [[Random Variable|Случайные величины]] $X$ и $Y$, определённые на [[Probability Space|вероятностном пространстве]] $(\Omega, \mathcal{F}, P)$, называются **независимыми** (*independent random variables*), если для любых борелевских множеств $B_1, B_2 \in \mathcal{B}(\mathbb{R})$ выполняется:
> $$
> P(X \in B_1, Y \in B_2) = P(X \in B_1) \cdot P(Y \in B_2)
> $$
> Случайные величины $X$ и $Y$ называются **зависимыми** (*dependent random variables*), если они не являются независимыми, то есть существуют борелевские множества $B_1, B_2 \in \mathcal{B}(\mathbb{R})$ такие, что:
> $$
> P(X \in B_1, Y \in B_2) \neq P(X \in B_1) \cdot P(Y \in B_2)
> $$

>[!addition]- Связь зависимости с корреляцией и ковариацией
> Если $X$ и $Y$ независимы, то они нескоррелированы:
> $$
> X \perp Y \Rightarrow \text{Cov}(X,Y) = 0 \Rightarrow \rho_{X,Y} = 0
> $$
>     
> Это следует из того, что при независимости $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$.
> 
> Обратное неверно: некоррелированность не влечёт независимость: 
> $$
> \text{Cov}(X,Y) = 0 \not\Rightarrow X \perp Y
> $$
>>[!example]- Классический контрпример
>> Пусть $X \sim \mathcal{N}(0,1)$ и $Y = X^2$. Тогда:
>> $$
>> \text{Cov}(X,Y) = \mathbb{E}[X \cdot X^2] - \mathbb{E}[X]\mathbb{E}[X^2] = \mathbb{E}[X^3] - 0 \cdot 1 = 0
>> $$
>> так как $\mathbb{E}[X^3] = 0$ для стандартного нормального распределения (нечётный момент симметричного распределения).
>> 
>> Однако $X$ и $Y$ явно зависимы – зная $Y$, мы знаем $|X|$.
> 
>>[!warning]- Важное исключение
>> Для **совместно нормальных** (jointly normal) случайных величин некоррелированность эквивалентна независимости: 
>> $$
>> (X,Y) \text{ — совместно нормальные и } \rho_{X,Y} = 0 \Leftrightarrow X \perp Y
>> $$
>> Это одно из уникальных свойств многомерного нормального распределения.
> 
>>[!addition]- Интуитивное объяснение
>> - **Корреляция** измеряет только **линейную** зависимость между случайными величинами
>> - **Зависимость** — более общее понятие, включающее любые виды вероятностных связей
>> 
>> Некоррелированность означает отсутствие линейной связи, но может существовать нелинейная зависимость (как в примере с $Y = X^2$).

Если случайные величины $X$ и $Y$ независимы, то используется обозначение $X \perp Y$, так как $\perp$ – символ ортогональности.

[[Random Variable|Случайные величины]] $X$ и $Y$ **независимы** (*independent*) тогда и только тогда, когда для всех $x, y \in \mathbb{R}$: 

$$
F_{X,Y}(x,y) = F_X(x) \cdot F_Y(y)
$$

где $F_{X,Y}$ – [[Joint CDF|совместная функция распределения]], а $F_X$, $F_Y$ – [[Marginal CDF|маргинальные функции распределения]].

---

[[Continuous Random Variable|Абсолютно непрерывные случайные величины]]$X$ и $Y$ **независимы**, если для всех $x,y∈ℝ$

$$
f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y)
$$
где $f_{X,Y}$ – [[Joint PDF|совместная плотность вероятности]], а $f_X$ и $f_Y$ – [[Marginal PDF|маргинальные плотности вероятностей]].

---

[[Discrete Random Variable|Дискретные случайные величины]] $X$ и $Y$ **независимы**, если для всех $x,y∈ℝ$

$$
p_{X,Y}(x,y)=p_X(x) · p_Y(y)
$$

где $p_{X,Y}$ – [[Joint PMF|совместная функция вероятности]], а $p_X$ и $p_Y$
– [[Marginal PMF|маргинальные функции вероятности]].

>[!addition]- Взаимно независимые случайные величины
>![[Mutually and Pairwise Independent RVs|no-t]]

>[!example]- Пример независимых случайных величин
> Пусть проводятся два независимых бросания правильной монеты. Определим:
> 
> - $X$ – число выпавших орлов в первом броске (0 или 1)
> - $Y$ – число выпавших орлов во втором броске (0 или 1)
> 
> Тогда для любых $i, j \in {0, 1}$: 
> $$
> P(X = i, Y = j) = \frac{1}{4} = \frac{1}{2} \cdot \frac{1}{2} = P(X = i) \cdot P(Y = j)
> $$

>[!example]- Пример зависимых случайных величин
> Пусть $X$ – случайная величина с нормальным распределением $X \sim \mathcal{N}(0, 1)$. Определим:
> $$
> Y = X^2
> $$
> Тогда случайные величины $X$ и $Y$ зависимы. Например:
> $$
> P(X > 0, Y > 1) = P(X > 1) = 1 - \Phi(1) \approx 0.159
> $$
> где $\Phi$ – функция распределения стандартного нормального распределения. Но 
> $$
> P(X > 0) = 0.5, \quad P(Y > 1) = P(|X| > 1) = 2(1 - \Phi(1)) \approx 0.317
> $$
> Следовательно: 
> $$
> P(X > 0, Y > 1) \approx 0.159 \neq 0.5 \times 0.317 \approx 0.159
> $$
> Фактически, $P(X > 0, Y > 1) = \frac{1}{2}P(Y > 1)$ – случайные величины $X$ и $Y$ зависимы.

>[!example]- Более сложный пример зависимости
> Пусть $(X, Y)$ имеют двумерное нормальное распределение (bivariate normal distribution) с корреляцией $\rho \neq 0$:
> $$
> f_{X,Y}(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}(x^2 - 2\rho xy + y^2)\right)
> $$
> При $\rho \neq 0$ случайные величины $X$ и $Y$ зависимы, так как: 
> $$
> f_{X,Y}(x,y) \neq f_X(x) \cdot f_Y(y) = \frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}
$$

