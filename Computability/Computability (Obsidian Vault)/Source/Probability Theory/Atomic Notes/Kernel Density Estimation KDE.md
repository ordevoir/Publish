
**Ядерная оценка плотности** (*Kernel Density Estimation*, KDE) – это непараметрический способ восстановить [[Probability Density Function PDF|непрерывную плотность вероятности]] по выборке данных, не предполагая заранее форму распределения. Идея заключается в том, что каждое наблюдение «размазывается» гладким колоколом (ядром), а итоговая плотность – сумма этих колоколов:

$$
\hat f_h(x)=\frac{1}{n h}\sum_{i=1}^{n} K\!\left(\frac{x-x_i}{h}\right),
$$
где $h>0$ – ширина окна (*bandwidth*), $K(\cdot)$ – ядро, обычно гауссово [[Standard Normal Distribution|❐]]:

$$
K(t) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}t^2}
$$
>[!example]-
>![[Kernel_Density_Estimation.ipynb|Kernel_Density_Estimation]]

>[!addition]- Применение KDE
>    * Визуализация распределения.
>    * Оценка мод, плотности, вероятностей в интервалах.
>    * Детекция аномалий/novelty detection (через уровни плотности).

>[!warning]- Выбор ширины окна $h$ – критичен
> - Слишком маленький $h$: шум, «зубчатая» плотность.
> - Слишком большой $h$: переcглаживание, потеря структуры.
> - Правила: Silverman’s rule, Scott’s rule, кросс-валидация, plug-in.

>[!tip]- Выбор ядра
>    Часто не так важен, как $h$. Популярны: Gaussian, Epanechnikov, Top-hat, Cosine. Обычно берут Gaussian «по умолчанию».

>[!warning]- KDE на границах и проклятие размерности
>    На концах диапазона KDE может «утекать» за пределы (например, для неотрицательных величин). Используют отражение данных, boundary kernels и т.п.
> 
>    В многомерном случае KDE страдает от «проклятия размерности»: нужно очень много данных. Возможны диагональные/полные матрицы сглаживания.

