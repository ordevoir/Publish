# Неопределённость

**Неопределённость** (_uncertainty_) – это мера отсутствия информации о каком-либо исходе. Чем больше неопределённость, тем менее предсказуем результат, и тем больше информации требуется для устранения этой неопределённости. В контексте теории информации неопределённость возникает, когда существует несколько возможных исходов, и до получения сообщения неизвестно, какой из них произойдёт. Количественная мера неопределенности определяется [[#Энтропия СВ|энтропией]].

Удобно рассуждать в терминах [[Random Variable|случайных величин]] (СВ), в частности, если мы имеем дело с [[Information Coding#Символы и сообщения как СВ|символами или сообщениями]].

**Устранение неопределенности** (_elimination of uncertainty_) заключается в сокращении числа возможных вариантов до 1.

Неопределённость можно интерпретировать как степень нашего "незнания" о том, что произойдёт, до того, как мы получим соответствующую информацию. Чем больше возможных исходов и чем ближе их вероятности друг к другу, тем выше неопределённость.

>[!example]-
>Если у нас есть справедливая монета, то до броска монеты существует полная неопределённость (с максимальной энтропией) относительно того, что выпадет, поскольку оба исхода равновероятны. После того, как будет получен результат испытания, неопределенность исчезает полностью – число возможных вариантов сократится до 1.

# Информация 

В теории информации, информация – это величина, которая характеризует **сокращение неопределённости** (_reduction of uncertainty_) после получения сообщения. Шеннон предложил способ измерения количества информации в битах, при котором 1 бит информации устраняет неопределенность между двумя равновероятными и взаимно исключающими возможными исходами. 

## Равновероятные исходы

Для броска монеты, при котором существует два равновероятных варианта исхода $p_1 = p_2 = 0.5$, информация о результате устраняет неопределенность, и потому, содержит 1 бит информации. Два варианта с равными вероятностями сократились до одного варианта.

Если к примеру, есть 4 равновероятных варианта исхода, например, следующий нуклеотид при секвенировании, то информация об исходе сокращает неопределенность в 4 раза. Для этого требуется 2 бита информации: первый бит сокращает неопределенность до двух нуклеотидов, а второй бит – сокращает до одного нуклеотида.

В более общем случае, если имеется $m$ равновероятных исхода, то сообщение об исходе сокращает неопределенность в $m$ раз. Если предположить, что $m$ равно некоторой целочисленной степени двух, т.е. $m=2^n$, то для того, чтобы устранить неопределенность, потребуется $\log_2 m$ бит.

>[!example]-
>Пусть СВ имеет 8 равновероятных исхода ($m=8$). Для устранения неопределенности потребуется $\log_2 8 = 3$ бита: первый бит сократит неопределенность вдвое, и останется 4 варианта исхода, второй бит сократит число вариантов до 2, а третий бит – до 1.

Можно обобщить эти рассуждения и на произвольные целочисленные значения $m$. В этих случаях значение $\log_2 m$ может уже оказаться не целочисленным. 

>[!example]-
>Пусть имеется 3 равновероятных варианта исхода ($p=1/3$). Тогда, сообщение конкретном исходе (не важно, каком) будет содержать $\log{3}=1.58$ бит информации, так как оно сокращает неопределенность с 3 вариантов до 1.

Вероятности $p$ равновероятных исходов равны $1/m$, поэтому количество информации, которое несет в себе сообщение о том или ином исходе, можно можно вычислять через вероятность: $\log_2{\frac{1}{p}} = -\log_2 p$ бит.
## Не равновероятные исходы

Дальнейшее обобщение можно сделать из следующих соображений: если число вариантов сокращается с $m$ равновероятных вариантов до $n$ равновероятных вариантов, то неопределенность сокращается в $m/n$ раз. 
 
Рассмотрим ситуацию, когда возможны два варианта исхода $A$ и $B$ с разными(!) вероятностями $p_A=0.75, \; p_B=0.25$.Сколько информации будет содержать сообщение об исходе $A$, или сообщение об исходе $B$? Для ответа на этот вопрос можно представить, что мы имеем дело с 4 равновероятными исходами: $A_1, A_2, A_3$ и $B$, с вероятностями $p=0.25$, при котором события $A_1, A_2, A_3$ однозначно сводятся к событию $A$. Сообщение об исходе $B$ сокращает неопределенность в 4 раза, так как число равновероятных вариантов сокращается с 4 до 1 ($m=4, n=1$). Сообщение о событии $A$ сокращает неопределенность с 4 равновероятных вариантов до 3 равновероятных, т.е. в 4/3 раза ($m=4, n=3$).

Таким образом, сообщение о наступлении события $A$ с вероятностью $p_A=0.75$ содержит $\log_2 {(4/3)} \approx 0.41$ бит информации, а сообщение о $B$ (вероятность которого $p_B=0.25$): $\log_2{(4/1)}=2$ бит. 

Как и в случае с равновероятными исходами, в данном случае, информация, которую несет в себе сообщение об исходе, может быть вычислена из вероятности исхода: $-\log_2 p_B = 2$, $-\log_2 p_A \approx 0.41$.  

>[!addition]-
>В случае двух вариантов исхода мы можем интуитивно предположить, что наибольшая неопределенность имеется при равных вероятностях, так как любое отклонение от этого равновесия делает исход более предсказуемым. В пределе, когда вероятность события A стремится к 0, а вероятность B, соответственно, к 1, мы можем считать исход вполне определенным, и потому, сообщение о том, что произошло событие B не является информативным.
>
>В то же время, если вероятность события мала, то сообщение о том, что произошло это событие, несет в себе много информации. Таким образом, можно заключить, что сообщение несет в себе тем больше информации, чем меньше его вероятность.

В общем случае, мы можем говорить, что если событие $x_i$ некоторой дискретной [[Random Variable|случайной величины]] (СВ) $X$, имеет вероятность $p_i$, то количество информации, содержащееся в сообщении о том, что произошло определенное событие $x_i$, определяется формулой

$$
I_i = -\log{p_i}
$$
^outcome-information

## Среднее количество информации

Сколько информации в среднем содержится в сообщении об исходе, когда возможны два варианта исхода $A$ и $B$ с разными вероятностями $p_A=0.75, \; p_B=0.25$? Сообщение об исходе $A$ содержит около $0.41$ бит информации, а сообщение об исходе $B$ – $2$ бита. Для того, чтобы посчитать среднее, мы должны учесть, что событие $A$ будет происходить чаще, чем, событие $B$, так что вместо арифметического среднего, необходимо брать взвешенное среднее. Если имеется серия исходов данной СВ, то относительные частоты исходов $A$ и $B$ будут распределятся в соответствии с их вероятностями, поэтому, весами служат вероятности:

$$
\overline{I} = 0.75 \cdot 0.41 + 0.25 \cdot 2 \approx 0.81 \text{ bit}
$$

В общем случае, зная распределение вероятностей дискретной СВ, мы можем вычислить среднее значение количества информации в сообщении об исходе по формуле Шеннона:

$$
\overline I = - \sum_i p_i \log_2{p_i}
$$

## Последовательность исходов

Пусть имеется некоторая последовательность $n$ [[Independent Identically Distributed IID|одинаково распределенных независимых СВ]] $X_1, X_2, \ldots, X_n$ (где $X_i = X$). Результатом испытания (*outcome*) такой СВ является последовательность некоторых значений $u_1, u_2, \ldots, u_n$, где $u_k$ принадлежит множество возможных значений СВ $X$. Такую последовательность можно получить, к примеру, произведя $n$ испытаний СВ $X$. 

Количество информации, содержащееся в исходе $u_1, u_2, \ldots, u_n$ равно суммарной информации отдельных исходов $u_k$:

$$
I = - \sum_i \log_2 {p(u_k)}
$$
В частности, $u_1, u_2, \ldots, u_n$ может представлять собой сообщение, составленное из символов, излучаемых некоторым случайным источником.

Здесь следует подчеркнуть, что речь идет о независимых СВ: очередной исход  $u_k$ в последовательности никак не зависит от предыдущих (и последующих) исходов.

>[!example]
>Для СВ, с возможными исходами $A$ и $B$ с вероятностями $p_A=0.75, \; p_B=0.25$, подсчитаем, сколько информации содержится в последовательность $AABABAB$. Исход $A$ содержит $I_A = - \log_2 {p_A} = 0.41$ бит информации, а исход $B$ содержит $I_B = - \log_2 {p_B} = 2$ бит. В последовательности содержится 4 исхода $A$ и 3 исхода $B$, поэтому содержит 
>
>$$
>I = 4 \cdot 0.41 + 3 \cdot 2 = 7.64 \; \text{bit}
>$$

# Энтропия СВ

**Энтропия** – это количественная мера неопределённости [[Random Variable|случайной величины]]. Она характеризует среднее количество информации $\overline I$, необходимое для устранения неопределённости в исходе. Пусть СВ $X$ принимает значения из множества $\{x_1, x_2, \dots, x_n\}$ с вероятностями $p_1, p_2, \dots, p_n$. Тогда, энтропия $H$ случайной величины $X$ определяется по формуле Шеннона (т.е. $H(X) = \overline I$):

$$
H(X) = - \sum_{i=1}^{n} p_i \log_2 p_i
$$
^entropy-formula

Энтропия измеряется в **битах**, если логарифм по основанию 2, и в **нат**ах, если используется натуральный логарифм по.

После того, как будет получен результат испытания, неопределенность исчезает полностью: мы получили один определенный результат $x$ и его вероятность равна 1 (энтропия $H = - 1 \cdot \log_2 1 = 0$). 

Конкретный исход $x_i$ в испытании содержит информацию $I_i$ в соответствии с его вероятностью $p_i$, которая определяется [[#^outcome-information|формулой]], полученной выше. 

Понятие энтропии применяется к случайным величинам, в то время как понятие информации применяется к определенным исходам случайной величины. 

>[!code]-
>
>```python
>def get_entropy(distribution):
>    return - (distribution * np.log2(distribution)).sum()
>
>X = { "xs": np.array([1.0,  2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0 ]),
>	  "ps": np.array([0.35, 0.35, 0.10, 0.10, 0.04, 0.04, 0.01, 0.01]) }
>
>print(get_entropy(X["ps"]))
>```

## Диапазон значений энтропии

Значение энтропии может меняться в пределах от $0$ до $\log_2n$ бит, где $n$ – количество возможных исходов [[Discrete Random Variable|дискретной]] СВ. Значение $0$ соответствует полной предсказуемости, а значение $\log_2n$ – полной (максимальная) неопределенности.

Если все исходы [[Discrete Random Variable|дискретной]] СВ равновероятны, энтропия максимальна, потому что неопределённость максимальна. Например, для равновероятной монеты энтропия составляет $\log_22=1$ бит.

Если один из исходов имеет вероятность, близкую к 1, а остальные — очень малые вероятности, энтропия будет низкой, так как неопределённость почти отсутствует.

>[!example]-
>1. При бросании честной монеты мы имеем два исхода с вероятностями $p_1 = p_2 = 0.5$. Энтропия(неопределенность) при бросании монеты равна
>$$
>H = - (0.5 \log_2{0.5} + 0.5 \log_2{0.5}) = -(2 \cdot 0.5 (-1)) = 1 \; \text{bit}
>$$
>Это максимально возможное значение энтропии при $n = 2$ (т.к. $\log_2{2}=1$), т.е. имеем полную неопределенность.
> 
>1. При бросании шестигранного кубика, возможно 6 равновероятных исходов, с $p=1/6$. Энтропия при бросании кубика равна
>$$
>H = - \sum_{i=1}^6 p \log_2 p = -6 \frac{1}{6} \log_2 \frac{1}{6} = \log_2 6 \approx 2.58 \; \text{bit}
>$$
>В данном испытании мы так же имеем полную неопределенность, так как энтропия равна максимально возможному значению $\log_2 n$. 
> 
>1. При секвенировании ДНК, если появление нуклеотидов равновероятны ($p=1/4$), то неопределённость в определении одного нуклеотида равна:
>$$
>H = \log_2 4 = 2 \text{ bit}
>$$
>В этом примере мы так же имеем полную неопределенность.
> 
>1. Предположим, что вероятность дождя завтра составляет 30%, а вероятность того, что будет солнечно — 70%. Энтропия для двух возможных исходов
>$$
>H = - (0.3 \log_2 0.3 + 0.7 \log_2 0.7) \approx 0.881 \text{ bit}
>$$
>В это примере уже нет полной неопределённости, так как возможные исходы не равновероятны, и энтропия меньше, чем максимальна возможная при $n = 2$.
>
sdaasdasdasd

# Энтропия последовательности СВ

## IID RV

Если имеется последовательность $n$ [[Independent Identically Distributed IID|одинаково распределенных независимых СВ]] $X_1, X_2, \ldots, X_n$, то для устранения неопределенности, необходимо в $n$ раз больше информации, чем то количество информации, которое требуется для устранения неопределенности для исхода одной СВ (в среднем):

$$
H(X_1, \ldots, X_n) = n \cdot H(X) = -n \sum_{i=1}^m p_i \log_2 p_i
$$



$$

$$
