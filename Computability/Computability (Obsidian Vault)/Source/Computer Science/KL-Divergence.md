
**Относительная энтропия** (*Kullback-Leibler divergence*) $D_{\mathrm{KL}}(P \,||\, Q)$ представляет собой метрику для вычисления дистанции между двумя распределениями $p(x)$ и $q(x)$ случайных величин $P$ и $Q$. Можно произвести разложение [[Cross-Entropy|кросс-энтропии]]:

$$
H(P, Q) = H(P) + D_{\mathrm{KL}}(P \,||\, Q)
$$

Кросс-энтропия $H(P, Q)$ всегда больше [[Information Entropy|энтропии]] $H(P)$, поэтому $D_{\mathrm{KL}}(P \,||\, Q)$ является неотрицательной величиной. При одинаковых распределениях  $H(P, Q)$ становится равной $H(P)$, и $D_{\mathrm{KL}}(P \,||\, Q)$ обращается в ноль. 

Относительную энтропию можно выразить через $p(x)$ и $q(x)$:

$$
D_{\mathrm{KL}}(P \,||\, Q) = \sum_i p(x_i) \log_2 \frac{p(x_i)}{q(x_i)}
$$

Следует иметь в виду, что относительная энтропия (как и кросс-энтропия) не является симметричной: в общем случае $D_{\mathrm{KL}}(P \,||\, Q)$ не равно  $D_{\mathrm{KL}}(Q \,||\, P)$ и не удовлетворяет неравенству треугольника. 

