
>[!abstract]
>**Кодирование** (*coding*, *encoding*) – это процесс преобразования сообщений в  специфическую последовательности символов, обеспечивающую эффективную, надежную и безопасную передачу или хранение сообщения. При этом обычно используется компрессия и исправление ошибок, а так же может иметь место и шифрование.
^encoding
# Базовые понятия

**Сообщение** (*message*) представляет собой последовательность [[#^alphabet|символов]] $u_1, u_2, \ldots, u_n$ (где $u_j \in I_m$), сгенерированная источником и предназначенная для передачи по каналу связи. В частности, сообщение может состоять из одного символа (или сигнала). В более широком смысле, сообщение – это любая форма данных, переданная от **отправителя** (*sender*, *transmitter*) **получателю** (*receiver*).
^message

**Символы** (*symbols*) представляют собой базовый компонент сообщений, и являются дискретными единицами заранее определенного множества $I_m$ (состоящего из $m$ элементов), называемого **алфавитом** (*alphabet*).
^alphabet

**Источник данных** (*source*) испускает сообщения – **исходные данные** (*source data*), которые представляют собой сырую информацию. Сообщение **длины** $n$ (называют еще *n-string* или *n-word*) – это последовательность $\mathbf u^{(n)}=u_1 u_2 \dots u_n$ элементов из алфавита $I_m$.
^data-source

**Код** представляет собой отображение (*map*) $f:I_m \to J^*$, которое переводит символы $u \in I_m$ в конечную строку: $f(u)=x_1,\ldots,x_s$, где $x_i \in J$. В цифровой коммуникации используется как правило бинарный алфавит $J = \{0, 1\}$.
^code

**Кодер** (*coder or encoder*) использует множество $J_q$ (алфавит из $q$ элементов) для преобразования исходных данных. Обычно $q<m$ или даже $q<<m$, а во многих случаях, $q=2$ с $J_2=\{0, 1\}$, когда используется **бинарный кодер** (*binary coder*).
^coder

**Кодированные данные** (*encoded data*) получаются в результате работы кодера и представляют **закодированное сообщение** (*coded message*).  

Кодированные последовательности, которые являются образами $f(u)$, называются **кодовыми словами** (*codewords*). Закодированное сообщение $f(\mathbf u^{(n)})=f(u_1)f(u_2)\ldots f(u_n)$ исходного сообщения $\mathbf u^{(n)}=u_1 u_2 \dots u_n$ также является последовательностью (вектором) пространства $J^*$.
^codewords

Код может иметь **постоянную длину** (*constant length*) $N$, если, для каждого прообраза $u$ функции $f$ длина кодового слова $f(u)$ одинакова и равна $N$. В общем случае, длины кодовых слов могут отличаться.
^constant-length

# Префиксный код

Кодер производит **кодирование без потерь** (*lossless*), если из $u \neq u'$ следует $f(u) \neq f(u')$, т.е. отображение $f:I_m \to J^*$ является взаимно однозначным (*one-to-one*). 

Код называется **дешифруемым** (*decipherable*), если любая последовательность из $J^*$ является образом не более одного сообщения. Сообщение, закодированное дешифруемым кодом, принципиально может быть восстановлено декодером без потерь.
^decipherable

Последовательность $x$ является **префиксом** (*prefix*) другой последовательности $y$, если $y=xz$, т.е. $y$ может быть представлена как результат конкатенации $x$ и $z$. Код называется **префиксным** (*prefix-free*), если нет [[#^codewords|кодовых слов]], которые являются префиксами других [[#^codewords|кодовых слов]] (например, код с постоянной длиной является префиксным).
^prefix-free

Всякий префиксный код является дешифруемым, но не всякий дешифрируемый код является префиксным. 

>[!example]-
>Пусть исходный алфавит состоит из символов $1, 2, 3$. Бинарный кодер с функцией $f(1) = 0, \; f(2) = 01, \; f(3) = 011$, является дешифруемым, но не является префиксным. Можно заметить, что в его [[#Кодовое дерево|кодовом дереве]] не все [[#^codewords|кодовые слова]] соответствуют листьям:
>
>```
>	  011
>	 /
>	01 
>	 \ 
>	  0     
>	   \  
>	     .
>```
>Тем не менее, дешифровать сообщение, закодированное этим кодом можно. Например, сообщение 132312 будет закодировано в `001101011001`. Первый символ `0` определенно соответствует 1, но далее не понятно сразу, идет ли вслед за этим 1, 2 или 3: мы могли бы считать следующие два символа `01` и декодировать в 2. Однако, при дальнейшем считывании мы обнаружим, что следующее кодовое слово должно начинаться с `1`, хотя в нашем коде нет таких [[#^codewords|кодовых слов]]. Из этого можно сделать вывод о том, что мы неверно декодировали второй символ исходного сообщения, вернуться на шаг назад и попробовать другое кодовое слово. Во всяком случае, дешифровать сообщение нам удастся.

Преимуществом префиксного кода является то, что при дешифровании можно последовательно идти слева направо и однозначно декодировать символы сообщения, так как ни одно кодовое слово не является началом другого кодового слова.

>[!addition]-
>Любой код с [[#^constant-length|постоянной длиной]] [[#^codewords|кодовых слов]] является префиксным кодом. 
>
>При кодировании префиксным кодом, [[#^codewords|кодовые слова]] соответствуют листьям кодового дерева.

# Символы как СВ

Появление cимвола в случайном процессе (или в сообщении) можно рассматривать как [[Discrete Random Variable|дискретную случайные величину]] (СВ) $X$, если перенумеровать элементы [[#^alphabet|алфавита]], т.е. отобразить на множество числовых значений. Далее можно рассматривать вероятности появления того или иного [[#^alphabet|символа]] ([[Random Variable|закон распределения]]). Следует при этом иметь в виду, что отношения порядка между числами будет фиктивным.

# Сообщения как СВ

[[#^message|Сообщения]] $\mathbf u^{(n)}=u_1 u_2 \dots u_n$ длины $n$ можно рассматривать как многомерную дискретную СВ $X_1, X_2, \ldots, X_n$. Если мы имеем дело со случайным [[#^data-source|источником]], который излучает [[#^alphabet|символы]] независимо, в соответствии с постоянным [[Random Variable|законом распределения]], то излучаемая последовательность символов (сообщения) может быть смоделирована последовательностью [[Independent Identically Distributed IID|одинаково распределенных независимых СВ]].

# Теорема Крафта

>[!theorem] Теорема Крафта
>
>Пусть даны целые положительные числа $s_1, \ldots, s_m$. Для существования [[#^decipherable|дешифруемого]] (в частности, [[#^prefix-free|префиксного]]) кода $f: I \to J^*$ с кодовыми словами, длины которых равны $s_1, \ldots, s_m$, необходимо и достаточно, чтобы выполнялось **неравенство Крафта**:
>
>$$
>\sum_{i=1}^m q^{-s_i} \leq 1
>$$
>
>где $q$ – число символов, используемых для кодирования.
^kraft-inequality

Следует отметить, что код, удовлетворяющий неравенству крафта, не обязательно будет дешифрируемым или, тем более, префиксным. Теорема утверждает лишь, что при данных условиях префиксный код существует, но наряду с тем, существует и не дешифрируемый код, и дешифрируемый не префиксный код.

Выполнение неравенства Крафта можно интерпретировать как необходимое и достаточное условие существования кодового дерева, имеющего $m$ листьев, глубина которых $s_1, \ldots, s_m$.

# Кодовое дерево

[[#Префиксный код. Теорема Крафта|Префиксный код]] может быть представлен в виде $q$-арного **кодового [[Trees#^tree|дерева]]** ([[Trees#^tree-degree|степень дерева]] равна мощность алфавита $q$), на [[Trees#^leaf|листьях]] которого располагаются [[#^codewords|кодовые слова]]. Длина кодового слова соответствует [[Trees#^node-depth|глубине]] соответствующего листа. [[#^codewords|Кодовые слова]], соответствующие [[Trees#^internal-node|внутренним узлам]], не образуют префиксный код. 

Возникает главный вопрос: какими свойствами должно обладать кодовое дерево, чтобы кодировать сообщение наиболее [[#Оптимальное кодовое дерево|оптимальным образом]]. 

## Идеальное кодовое дерево

Рассмотрим в качестве примера бинарное кодовое дерево. Простейшим вариантом будет идеальное дерево глубины 2, четырем листьям которого соответствуют [[#^codewords|кодовые слова]] (`00`, `01`, `10`, `11`) постоянной длины, равной 2 бит:

```
00 01 10 11
 \ /   \ /
  0     1
   \   /
     .
```

>[!detail]-
>В общем случае, когда используются [[#^codewords|кодовые слова]] с различными длинами, то дерево не будет идеальным, а его глубина будет соответствовать максимальной длине кодового дерева.

При помощи этих четырех [[#^codewords|кодовых слов]] мы можем кодировать, к примеру нуклеотиды ATGC, произведя соответствие A – `00`, T – `01`, G – `10`, C – `11`. Тогда, для кодирования последовательности из $n$ нуклеотидов, потребовалось бы $2n$ бит. Код является [[#^prefix-free|префиксным]], так как ни одно кодовое слово не является началом другого кодового слова.

Между узлами идеального бинарного дерева степени $m$ и всевозможными кодовыми словами с длинами $1 \leq s \leq m$ (из алфавита $\{0, 1\}$), устанавливается взаимно-однозначное соответствие. Причем, узлам степени $s_i$ соответствуют [[#^codewords|кодовые слова]] длины $s_i$. Таким образом каждое кодовое слово может служить адресом соответствующего узла.

## Полное кодовое дерево

Для кодирования трех различных вариантов исходов СВ, мы могли бы так же воспользоваться идеальным деревом, игнорируя один из листов (одно из [[#^codewords|кодовых слов]] было бы фиктивным). И в этом случае для кодирования последовательности из $n$ исходов СВ вариантов, потребовалось бы также $2n$ бит. Однако, здесь, для сокращения длины закодированного сообщения, мы могли бы воспользоваться не идеальным деревом, с тремя листами (`0`, `10`, `11`):

```
      10 11
       \ /
  0     1
   \   /
     .
```

Данный код также является [[#^prefix-free|префиксным]]. Если вероятности исходов СВ одинаковы, то и вероятности появления [[#^codewords|кодовых слов]] в закодированном сообщении, одинакова. Так что в среднем, один исход будет кодироваться кодовым словом, длиной $(1+2+2)/3 \approx 1.67$ бит.

## Выполнение неравенства Крафта

Убедимся, что в рассмотренных случаях выполняется [[#^kraft-inequality|неравенство Крафта]]. Используемый алфавит состоит из двух символов $\{0, 1\}$, так что $q=2$. 

В первом случае все [[#^codewords|кодовые слова]] имеют постоянную длину $s=2$, поэтому $q^{-s} = 1/4 \leq 1$ выполняется. Во втором случае кодовые слова имеют длины $s_1=1$, $s_2=2$, поэтому, неравенство Крафта также будет выполняться:

$$
q^{-s_1} + q^{-s_2} = \frac{1}{2} + \frac{1}{4} = \frac{3}{4} \leq 1
$$

# Оптимальное кодовое дерево

## Постановка задачи

Если кодовое дерево не является идеальным, то [[#^codewords|кодовые слова]], определяемые листьями дерева, имеют переменную длину. Если мы будем кодировать сообщения длины $n$, при помощи кода, определенного таким кодовым деревом, то в результате будем получать различные длины сообщений. В зависимости от закона распределения в алфавите исходного сообщения, и кода, мы будем получать некоторую среднюю длину кодового слова. В предположении, что сообщение представляет собой последовательность [[Independent Identically Distributed IID|независимых случайных величин с одинаковым распределением]], необходимо составить дерево с минимальной средней длиной кодового слова. Такой код позволит кодировать исходные сообщения с минимальной избыточностью.

## Энтропия сообщения

Здесь мы моделируем источник как дискретную случайную величину $X$ с возможными исходами $x_1, \ldots x_m$, с распределением $p_1, \ldots, p_m$. Тогда $H(X)$ представляет собой энтропию случайной величины $X$. Фактически, это среднее количество информации, которую несет информация об исходе случайной величины $X$.

Можно далее обобщить это на сообщения произвольной длины $n$, сгенерированные случайным [[#^data-source|источником]], которые можно моделировать последовательностью [[_Scaling Sum Product Mean#Одинаково распределенные СВ|одинаково распределенных независимых СВ]], т.е. $X_1, \ldots X_n$. Тогда энтропия сообщения будет равна $n\cdot H(X)$ ([[Information Entropy#IID RV|подробнее]]). Это неверно для сообщения, сгенерированного источником, который излучает символы зависящие от предыдущих символов.

## Оценка оптимальной длины

Код является оптимальным, если средняя длина его [[#^codewords|кодовых слов]] является минимально возможным. Теорема Шеннона определяет условие оптимальности кода в соответствии со средней длиной [[#^codewords|кодовых слов]].

>[!theorem] Теорема Шеннона об источнике шифрования
>Для случайного [[#^data-source|источника]] $X$, излучающего символы $u_1, u_2, \ldots u_i, \ldots$ с вероятностями $p_i > 0$, минимальная ожидаемая длина $L$ кодового слова для дешифруемого кодирования в бинарном алфавите $J=\{0, 1\}$ подчиняется неравенству
>
>$$
>H(X) \leq \min \mathbb E[L] < H(X) + 1
>$$
>
>где $H(X) = - \sum_{i=1}^{m} p_i \log_2 p_i$ – [[Information Entropy#Энтропия СВ|энтропия]] [[#^data-source|источника]].
>
>>*Shannon’s noiseless coding theorem (NLCT)*
^nlct

>[!example]-
При мощности алфавита $4$ энтропия СВ будет максимальна при равномерном распределении вероятностей и равна 2 бит. Тогда $2 \leq \min \mathbb E[L] = 3$. Как показано [[#Идеальное кодовое дерево|выше]], средняя длина кодового слова равна 2.

## Следствия теоремы Шеннона

Для того, чтобы закодировать возможные исходы случайной величины $X$, среднее содержание информации об исходах ([[Information Entropy#Энтропия СВ|энтропия СВ]]) которой равно $H(X)$ бит, потребуется код, средняя длина [[#^codewords|кодовых слов]] которой не менее $H(X)$. Это является следствием [[#^nlct|теоремы]] Шеннона. 

Еще одним следствием теоремы Шеннона является то, что для кодирования сообщения, содержащего $I$ бит [[Information Entropy#Последовательность исходов|информации]], некоторым бинарным кодом, потребуется не менее $I$ символов. В этом контексте говорят, что информация ведет себя как несжимаемая жидкость. 

Если кодирование производится оптимальным кодом, то в том случае, если $I$ является целочисленным, потребуется ровно $I$ символов. Если же $I$ является дробным, то потребуется  $\left\lceil I \right\rceil$ символов (округление вверх). И здесь мы имеем дело с другим следствием теоремы Шеннона: для кодирования сообщения, содержащего $I$ бит информации, оптимальным бинарным кодом, потребуется не более $I + 1$ символов. 

## Эвристика

[[#^codewords|Кодовые слова]] должны быть переменной длины, чтобы кодировать символы с наибольшей частотой короткими кодовыми словами. Для того, чтобы легко дешифровать, код должен быть префиксным. Кодовые слова, соответствующие наиболее вероятным символам, должны располагаться на как можно меньшей глубине. Внутренние узлы кодового дерева должны быть насыщенны, чтобы задействовать наибольшее возможное число коротких [[#^codewords|кодовых слов]]. Дерево должно быть полным, если это возможно.
# Код Хаффмана

Бинарный код Хаффмана получается в результате построения бинарного дерева. Для построения требуются таблица относительных частот (или вероятностей) символов алфавита $I_m$, упорядоченная по возрастанию частот. Дерево строится от листьев к корню: 
1. Все символы размещаются в узлах, которые в итоге будут листьями дерева. 
2. Отбирается пара узлов с наименьшими частотами и производится их слияние. Образуется родительский узел, вероятность которого равна сумме вероятностей дочерних узлов.
3. Отбор и слияние продолжается до тех пор, пока не образуется корень дерева, вероятность которого равна 1.

Код Хаффмана является оптимальным, так как средняя длина [[#^codewords|кодовых слов]] удовлетворяет неравенству [[#^nlct|теоремы]] Шеннона. 

Следует отметить, что средняя длина кодовых слов оптимального кода может не совпадать с энтропией. Однако она будет больше энтропии не более чем на 1 бит. В примере, рассмотренном ниже, ожидаемая длина кодовых слов оптимального бинарного кода несколько отличается от энтропии.

>[!addition]-
>Для того, чтобы на каждом шаге отбирать пару узлов, с наименьшими вероятностями, можно использовать очередь с приоритетами.
>
>Полученное кодовое дерево заведомо будет полным: можно считать, что мы начинаем с $m$ деревьев, состоящих из одного узла, и далее мы производим их слияние. Таким образом, на каждом этапе у нас имеется лес из полных деревьев.
>
>Код Хаффмана будет заведомо префиксным.

## Example

Рассмотрим случайный источник, который генерирует независимые символы в соответствии с некоторым законом распределения:

```python
X = { "xs": np.array(['a', 'b',  'c',  'd', 'e',  'f',   'g'  ]),
	  "ps": np.array([0.5, 0.15, 0.15, 0.1, 0.05, 0.025, 0.025]) }

```

Построим кодовое дерево для бинарного кода Хаффмана. Левая ветвь будет всегда `0`, а правая – `1` (хотя мы могли бы произвольно назначать разные значения при ветвлении).

![[huffman-tree.png|huffman]]
Соответствие (вероятностей $p_i$) символов и кодовых слов (и их длин $L_i$):

```
(0.5)   a -> 0      (1)
(0.15)  b -> 100    (3)
(0.15)  c -> 101    (3)
(0.1)   d -> 110    (3)
(0.05)  e -> 1110   (4)
(0.025) f -> 11110  (5)
(0.025) g -> 11111  (5)
```

```python
code = {'a': "0", 'b': "100", 'c': "101", 'd': "110", 
		'e': "1110", 'f': "11110", 'g': "11111" }
```

Математическое ожидание длины кодового слова (средняя длина)

$$
\mathbb E[L] = \sum_i p_i L_i = 2.15 \text{ bit}
$$

```python
def get_expected_length(X, code):
    dist = {str(s): float(p) for s, p in zip(X["xs"], X["ps"])}
    return sum([dist[key] * len(code[key]) for key in code.keys()])
	    
mean_L = get_expected_length(X, code)
print(mean_L)
```

Сравним с энтропией случайной величины `X`

$$
H(X) = - \sum_i p_i \log p_i \approx 2.135 \text{ bit}
$$

```python
def get_entropy(distribution):
    return - (distribution * np.log2(distribution)).sum()

print(get_entropy(X["ps"]))
```

Как видно, ожидаемая длина кодового слова $\mathbb E[L]$ немного больше энтропии $H(X)$. Однако, при бинарном кодировании, это оптимальная длина: никакой другой бинарный код не будет обладать меньшим значением ожидаемой длины.