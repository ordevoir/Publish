
Рассмотрим две случайные величины $P$ и $Q$ с одним и тем же набором возможных исходов $x_1, \ldots, x_m$, но различными распределениями вероятностей $p(x)$ и $q(x)$. Пусть случайный источник излучает символы в соответствии с $P$, и имеется код, оптимальный по отношению к $Q$, ожидаемая длина которого равна(!) [[Information Entropy|энтропии]] $Q$. Тогда кросс-энтропия распределения $q(x)$ по отношению к распределению $p(x)$ по заданному набору определяется как математическое ожидание длины кодового слова:

$$
H(P, Q) = - \mathbb E_p [\log_2 q] = - \sum_{i=1}^m p(x_i) \log_2 q(x_i)
$$
^cross-entropy-formula

Следует здесь иметь в виду, что оптимальный код может не совпадать с энтропией, и в соответствии с [[Information Coding#^nlct|теоремой]] Шеннона быть оказаться больше (но не более чем на 1 бит). В случае кода, оптимального для $Q$, но не равного в точности $H(Q)$, при кодировании символов $P$ ожидаемая длина кодового слова также может оказаться несколько больше, чем кросс-энтропия $H(P,Q)$. 

$$
\mathbb E_p[L] = \sum_{i=1}^m p(x_i) L_i 
$$
^expected-length-formula

где $L_i$ – длина кодового слова, кодирующего исход (символ) $x_i$. 

Кросс-энтропия не является симметричной: в общем случае, $H(P,Q)$ не равно $H(Q, P)$.

## Example

Возьмем в качестве $Q$ случайную величину, которая использовалась в [[Information Coding#Код Хаффмана#Example|примере]] для кода Хаффмана, соответствующий оптимальный код, построенный там же:

```python
Q = { "xs": np.array(['a', 'b',  'c',  'd', 'e',  'f',   'g'  ]),
	  "ps": np.array([0.5, 0.15, 0.15, 0.1, 0.05, 0.025, 0.025]) }

```

```python
code = {'a': "0", 'b': "100", 'c': "101", 'd': "110", 
		'e': "1110", 'f': "11110", 'g': "11111" }
```

>[!detail]-
>```
>(0.5)   a -> 0      (1)
>(0.15)  b -> 100    (3)
>(0.15)  c -> 101    (3)
>(0.1)   d -> 110    (3)
>(0.05)  e -> 1110   (4)
>(0.025) f -> 11110  (5)
>(0.025) g -> 11111  (5)
>```

Энтропия $Q$ равна $H(Q) = - \sum q_i \log q_i \approx 2.135 \text{ bit}$, а ожидаемая длина кодовых слов $\mathbb E_q = \sum q_i L_i = 2.15 \text{ bit}$.

В качестве $P$ возьмем другую СВ, определенную на том же множестве возможных значений, но с немного другим распределением $p(x)$:

```python
P = { "xs": np.array(['a',  'b',  'c',  'd',  'e',  'f',  'g'  ]),
	  "ps": np.array([0.35, 0.35, 0.10, 0.10, 0.04, 0.04, 0.02]) }
```

Его энтропия несколько выше энтропии $Q$ и составляет $H(P) = - \sum p_i \log p_i \approx 2.21 \text{ bit}$.

>[!code]-
>
>```python
>def get_entropy(distribution):
>    return - (distribution * np.log2(distribution)).sum()
>
>P_entropy = get_entropy(P["ps"])
>print(P_entropy)
>```

Кросс-энтропия распределения $p(x)$ по отношению к $q(x)$, вычисленная по [[#^cross-entropy-formula|формуле]], равна $H(P, Q) \approx 2.41 \text{ bit}$.

>[!code]-
>```python
>def get_cross_entropy(p, q):
>    return - (p * np.log2(q)).sum()
>
>cross_entropy = get_cross_entropy(P["ps"], Q["ps"])
>print(cross_entropy)
>```

Ожидаемая длина кодового слова кода `code`, при кодировании символов, которые распределены по закону $p(x)$, вычисленная по [[#^expected-length-formula|формуле]] равна $\mathbb E_p[L] = 2.46$.

>[!code]-
>```python
>def get_expected_length(X, code):
>    dist = {str(s): float(p) for s, p in zip(X["xs"], X["ps"])}
>    return sum([dist[key] * len(code[key]) for key in code.keys()])
>	    
>expected_length = get_expected_length(P, code)
>print(expected_length)
>```

Заметим, что ожидаемая длина кодового слова оказывается несколько больше кросс-энтропии. Это происходит по той же причине, по которой ожидаемая длина кодового слова оптимального кода может оказаться больше энтропии.

[[../../06 Machine Learning/Cross-Entropy Loss Function|❐]] Кросс-энтропия как функция потерь